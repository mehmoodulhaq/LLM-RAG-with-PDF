{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import redis\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Connect to Redis\n",
    "redis_client = redis.StrictRedis(host='localhost', port=6379, db=0, decode_responses=True)\n",
    "\n",
    "# Store data in Redis\n",
    "def store_in_redis(key, data):\n",
    "    redis_client.set(key, json.dumps(data))\n",
    "\n",
    "# Retrieve data from Redis\n",
    "def retrieve_from_redis(key):\n",
    "    data = redis_client.get(key)\n",
    "    if data:\n",
    "        return json.loads(data)\n",
    "    return None\n",
    "\n",
    "conversation_history_key = 'conversation_history'\n",
    "vectorstore_key = 'vectorstore'\n",
    "\n",
    "def load_pdf_and_create_vector_store(pdf_path):\n",
    "    \"\"\"Loads a PDF file, creates embeddings, and stores them in FAISS.\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    if not docs:\n",
    "        raise ValueError(\"üö® No documents were loaded from the PDF! Check if the file is valid.\")\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"deepseek-llm:7b-chat\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    \n",
    "    # Store vector store in Redis\n",
    "    store_vector_store_in_redis(vectorstore, vectorstore_key)\n",
    "    return vectorstore\n",
    "\n",
    "def store_vector_store_in_redis(vectorstore, key):\n",
    "    \"\"\"Serialize and store vector store in Redis.\"\"\"\n",
    "    vectorstore_data = vectorstore.save()  # Assuming FAISS has a `save()` method\n",
    "    store_in_redis(key, vectorstore_data)\n",
    "\n",
    "def retrieve_vector_store_from_redis(key):\n",
    "    \"\"\"Retrieve the vector store from Redis.\"\"\"\n",
    "    vectorstore_data = retrieve_from_redis(key)\n",
    "    if vectorstore_data:\n",
    "        return FAISS.load(vectorstore_data)  # Assuming FAISS has a `load()` method\n",
    "    return None\n",
    "\n",
    "def query_agent(question: str, vectorstore, llm):\n",
    "    \"\"\"Queries the vectorstore using the LLM and retrieves the answer.\"\"\"\n",
    "    try:\n",
    "        # Retrieve documents relevant to the current query\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "        retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"I don't know.\",\n",
    "                \"explanation\": \"No relevant information found in the documents.\",\n",
    "                \"source_documents\": []\n",
    "            }\n",
    "\n",
    "        context = \"\\n\".join([doc.page_content for doc in retrieved_docs if doc.page_content.strip()])\n",
    "\n",
    "        # Retrieve conversation history from Redis\n",
    "        global conversation_history\n",
    "        conversation_history = retrieve_from_redis(conversation_history_key) or []\n",
    "\n",
    "        if conversation_history:\n",
    "            # Join previous conversation history into the context\n",
    "            conversation_context = \"\\n\".join([f\"Q: {entry['question']}\\nA: {entry['answer']}\" for entry in conversation_history])\n",
    "            context = conversation_context + \"\\n\" + context\n",
    "\n",
    "        QA_PROMPT = PromptTemplate(\n",
    "            template=\"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say 'I don't know'.\n",
    "\n",
    "            Context: {context}\n",
    "            Question: {query}\n",
    "\n",
    "            Provide the answer in JSON format:\n",
    "            {{\n",
    "                \"answer\": \"your concise answer here\",\n",
    "                \"explanation\": \"your detailed explanation here\"\n",
    "            }}\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "        )\n",
    "\n",
    "        formatted_prompt = QA_PROMPT.format(context=context, query=question)\n",
    "\n",
    "        response = llm.invoke(formatted_prompt).strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_response = {\n",
    "                \"answer\": response,\n",
    "                \"explanation\": \"The model did not return a structured JSON response.\"\n",
    "            }\n",
    "\n",
    "        # Save the current question and answer to conversation history\n",
    "        conversation_history.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": parsed_response.get(\"answer\", \"No answer found\"),\n",
    "        })\n",
    "        \n",
    "        # Store updated conversation history in Redis\n",
    "        store_in_redis(conversation_history_key, conversation_history)\n",
    "\n",
    "        return {\n",
    "            \"answer\": parsed_response.get(\"answer\", \"No answer found\"),\n",
    "            \"explanation\": parsed_response.get(\"explanation\", \"No explanation provided\"),\n",
    "            \"source_documents\": [doc.page_content for doc in retrieved_docs if doc.page_content.strip()],\n",
    "            \"context\": conversation_history\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"Error occurred while processing the query\",\n",
    "            \"explanation\": str(e),\n",
    "            \"source_documents\": []\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm = OllamaLLM(model=\"deepseek-llm:7b-chat\")\n",
    "\n",
    "    # Load vectorstore from Redis or from PDF\n",
    "    vectorstore = retrieve_vector_store_from_redis(vectorstore_key)\n",
    "    if not vectorstore:\n",
    "        vectorstore = load_pdf_and_create_vector_store(\"English.pdf\")\n",
    "\n",
    "    query = \"please convert you answer into markdown and add headings to the last answer\"\n",
    "    result = query_agent(query, vectorstore, llm)\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
