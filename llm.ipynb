{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Ollama (if not installed)\n",
    "   Ollama is the easiest way to run DeepSeek on macOS.\n",
    "1. Download and install Ollama: <br>\n",
    "    ```curl -fsSL https://ollama.com/install.sh | sh```\n",
    "2. Verify installation: <br>\n",
    "    ```ollama version```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pull the DeepSeek Model\n",
    "   Ollama supports DeepSeek Coder and DeepSeek Chat models.\n",
    "*  For DeepSeek Coder (code generation & completion): <br>\n",
    "        ```ollama pull deepseek-coder```\n",
    "*  For DeepSeek Chat (general AI chat): <br>\n",
    "        ```ollama pull deepseek-chat```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 3: Run the DeepSeek Model\n",
    "    Once downloaded, you can run DeepSeek locally:\n",
    "\n",
    "*   Run an interactive chat session: <br>\n",
    "        ```ollama run deepseek-chat```\n",
    "*   Run a command-line query: <br>\n",
    "        ```ollama run deepseek-chat \"Explain LangChain with an example\"```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4: Use DeepSeek in Python (Optional)\n",
    "*    If you want to use DeepSeek in a Python script with Ollama API, install ollama in Python: <br>\n",
    "        ```pip install ollama```\n",
    "*   Then, call the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='deepseek-llm:7b-chat', messages=[{'role': 'user', 'content': 'What is LangChain?'}])\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me know if you need further customization, like using DeepSeek in LangChain! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can achieve this using LangChain with Ollama and a ReAct agent. Hereâ€™s how:\n",
    "\n",
    "#### **Approach**\n",
    "1. ***Upload & Parse PDF*** â€“ Use ```PyMuPDF``` or ```pdfplumber``` to extract text.\n",
    "2. ***Memory Handling*** â€“ Store extracted text in a retriever (e.g., FAISS, ChromaDB).\n",
    "3. ***ReAct Agent*** â€“ Create a LangChain agent with ***Ollama (DeepSeek or Mistral)*** as the LLM.\n",
    "4. ***Query Answering*** â€“ Use the extracted text as context for responses.\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Implementation Steps**\n",
    "1. **Install Dependencies** <br>\n",
    "        ```pip install langchain langchain-community ollama pypdf```\n",
    "2. **PDF Parsing & Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"English.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = FAISS.from_documents(docs, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Set Up Ollama ReAct Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import Tool\n",
    "\n",
    "# Use Ollama with DeepSeek\n",
    "llm = Ollama(model=\"deepseek-chat\")\n",
    "\n",
    "# Define the retriever as a LangChain Tool\n",
    "retriever_tool = Tool(\n",
    "    name=\"PDF Retriever\",\n",
    "    func=lambda query: retriever.get_relevant_documents(query),\n",
    "    description=\"Retrieves relevant information from the uploaded PDF.\"\n",
    ")\n",
    "\n",
    "# Initialize ReAct Agent\n",
    "agent = initialize_agent(\n",
    "    tools=[retriever_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.REACT_DOCSTORE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run a query\n",
    "response = agent.run(\"What is discussed in the first section of the PDF?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Enhancements**\n",
    "* **Use ChromaDB for faster retrieval**\n",
    "* **Use Mistral-7B if DeepSeek lacks context depth**\n",
    "* **Add ConversationalBufferMemory for multi-turn chats** <br>\n",
    "Would you like a Streamlit or FastAPI interface for this? ðŸš€\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ReActAgent' from 'langchain.agents.react' (/opt/anaconda3/lib/python3.12/site-packages/langchain/agents/react/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tool, AgentExecutor\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreact\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReActAgent, ReActOutputParser\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ReActAgent' from 'langchain.agents.react' (/opt/anaconda3/lib/python3.12/site-packages/langchain/agents/react/__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain.agents import Tool, AgentExecutor\n",
    "from langchain.agents.react import ReActAgent, ReActOutputParser\n",
    "from langchain.llms import Ollama\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ReAct Output Parser to ensure JSON formatting\n",
    "class StrictJSONOutputParser(ReActOutputParser):\n",
    "    def parse(self, text):\n",
    "        try:\n",
    "            # Attempt to parse the output into JSON\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON parsing fails, return a structured JSON with an error explanation\n",
    "            return {\n",
    "                \"answer\": \"\",\n",
    "                \"explanation\": f\"Output parsing error: Invalid JSON output received - {text}\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample function for the tool (replace with actual function)\n",
    "def pdf_retriever_function(input_data):\n",
    "    # This is a placeholder function, implement as needed for your use case\n",
    "    return \"Relevant data from the PDF document.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools (if needed)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"PDF Retriever\",\n",
    "        func=pdf_retriever_function,  # Replace with actual function\n",
    "        description=\"Retrieve relevant information from the uploaded PDF\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt to enforce strict JSON output\n",
    "system_prompt = \"\"\"You are an AI assistant using the ReAct framework.\n",
    "When answering a question, always return a strict JSON response in the following format:\n",
    "{\n",
    "  \"answer\": \"concise response to the question\",\n",
    "  \"explanation\": \"optional extra details if needed, else empty string\"\n",
    "}\n",
    "Make sure that the response is **only** valid JSON. Do not output any plain text or extra characters. Any invalid response should return an error message in the explanation field, explaining why the response is not valid JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Ollama model (replace with your model identifier)\n",
    "model = Ollama(model=\"deepseek-llm:7b-chat\")\n",
    "\n",
    "# Define the ReAct agent with the custom output parser\n",
    "output_parser = StrictJSONOutputParser()\n",
    "agent = ReActAgent(\n",
    "    tools=tools,\n",
    "    output_parser=output_parser,\n",
    "    llm=model,  # Use the Ollama model\n",
    "    system_message=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent executor\n",
    "executor = AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a PDF and create a vector store\n",
    "def load_pdf_and_create_vector_store(pdf_path):\n",
    "    # Load the PDF document using PyPDFLoader\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Create embeddings (you can replace OpenAIEmbeddings with any other embeddings you are using)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Create FAISS vector store\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query the agent and ensure JSON response\n",
    "def query_agent(question):\n",
    "    # Run the agent with the provided question\n",
    "    response = executor.run({\"input\": question})\n",
    "    \n",
    "    # Log the raw response for debugging\n",
    "    print(\"Raw response from agent:\", response)\n",
    "    \n",
    "    # Ensure the response is valid JSON (it will be after parsing)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PDF file\n",
    "pdf_path = \"English.pdf\"  # Replace with your PDF path\n",
    "\n",
    "# Load the PDF and create the vector store\n",
    "vectorstore = load_pdf_and_create_vector_store(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"What is the role of counselling in preventing drug addiction?\"\n",
    "response_json = query_agent(query)\n",
    "\n",
    "# Print the structured JSON response\n",
    "print(\"Structured JSON response:\", json.dumps(response_json, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
