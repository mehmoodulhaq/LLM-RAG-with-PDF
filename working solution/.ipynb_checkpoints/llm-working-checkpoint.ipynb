{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 148 documents from PDF.\n",
      "üîπ Document 1:\n",
      "\n",
      "---\n",
      "üîπ Document 2:\n",
      "\n",
      "---\n",
      "üîπ Document 3:\n",
      "\n",
      "---\n",
      "üîé Retrieved 5 documents for query: How can we become a strong nation? Please do not add any new line character symbols\n",
      "üìú Doc 1:\n",
      "\n",
      "---\n",
      "üìú Doc 2:\n",
      "\n",
      "---\n",
      "üìú Doc 3:\n",
      "\n",
      "---\n",
      "{\n",
      "  \"answer\": \"Focus on building a strong economy, investing in education and infrastructure, promoting unity among citizens, and engaging with international partnerships.\",\n",
      "  \"explanation\": \"A strong nation is built upon several key pillars. Firstly, economic growth is crucial to ensure the stability and well-being of its citizens. This can be achieved by fostering innovation, attracting investment, and creating job opportunities. Secondly, investing in education allows a nation to develop its human capital, promoting critical thinking and problem-solving skills.\",\n",
      "  \"source_documents\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def load_pdf_and_create_vector_store(pdf_path):\n",
    "    \"\"\"Loads a PDF file, creates embeddings, and stores them in FAISS.\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # ‚úÖ Debugging: Check if PDF loaded correctly\n",
    "    print(f\"üìÑ Loaded {len(docs)} documents from PDF.\")\n",
    "    for i, doc in enumerate(docs[:3]):  # Print first 3 documents for verification\n",
    "        print(f\"üîπ Document {i+1}:\\n{doc.page_content[:500]}\\n---\")\n",
    "\n",
    "    if not docs:\n",
    "        raise ValueError(\"üö® No documents were loaded from the PDF! Check if the file is valid.\")\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"deepseek-llm:7b-chat\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def query_agent(question: str, vectorstore, llm):\n",
    "    \"\"\"Queries the vectorstore using the LLM and retrieves the answer.\"\"\"\n",
    "    try:\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # ‚úÖ Fetch more documents\n",
    "        retrieved_docs = retriever.invoke(question)  # ‚úÖ Use `invoke()` instead of deprecated method\n",
    "\n",
    "        # ‚úÖ Debugging: Print retrieved documents\n",
    "        print(f\"üîé Retrieved {len(retrieved_docs)} documents for query: {question}\")\n",
    "        for i, doc in enumerate(retrieved_docs[:3]):  # Print first 3 for verification\n",
    "            print(f\"üìú Doc {i+1}:\\n{doc.page_content[:500]}\\n---\")\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"I don't know.\",\n",
    "                \"explanation\": \"No relevant information found in the documents.\",\n",
    "                \"source_documents\": []\n",
    "            }\n",
    "\n",
    "        context = \"\\n\".join([doc.page_content for doc in retrieved_docs if doc.page_content.strip()])\n",
    "\n",
    "        QA_PROMPT = PromptTemplate(\n",
    "            template=\"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say 'I don't know'.\n",
    "\n",
    "            Context: {context}\n",
    "            Question: {query}\n",
    "\n",
    "            Provide the answer in JSON format:\n",
    "            {{\n",
    "                \"answer\": \"your concise answer here\",\n",
    "                \"explanation\": \"your detailed explanation here\"\n",
    "            }}\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "        )\n",
    "\n",
    "        formatted_prompt = QA_PROMPT.format(context=context, query=question)\n",
    "\n",
    "        # ‚úÖ Send to LLM and ensure JSON parsing\n",
    "        response = llm.invoke(formatted_prompt).strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_response = {\n",
    "                \"answer\": response,\n",
    "                \"explanation\": \"The model did not return a structured JSON response.\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"answer\": parsed_response.get(\"answer\", \"No answer found\"),\n",
    "            \"explanation\": parsed_response.get(\"explanation\", \"No explanation provided\"),\n",
    "            \"source_documents\": [doc.page_content for doc in retrieved_docs if doc.page_content.strip()]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"Error occurred while processing the query\",\n",
    "            \"explanation\": str(e),\n",
    "            \"source_documents\": []\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"English.pdf\"  # Make sure this path is correct\n",
    "    vectorstore = load_pdf_and_create_vector_store(pdf_path)\n",
    "\n",
    "    llm = OllamaLLM(model=\"deepseek-llm:7b-chat\")\n",
    "\n",
    "    query = \"How can we become a strong nation? Please do not add any new line character symbols\"\n",
    "    result = query_agent(query, vectorstore, llm)\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 148 documents from PDF.\n",
      "üîπ Document 1:\n",
      "\n",
      "---\n",
      "üîπ Document 2:\n",
      "\n",
      "---\n",
      "üîπ Document 3:\n",
      "\n",
      "---\n",
      "üîé Retrieved 5 documents for query: Write down the summary of the poem *Stopping by the Woods on a Snowy Evening* by Robert Frost\n",
      "üìú Doc 1:\n",
      "\n",
      "---\n",
      "üìú Doc 2:\n",
      "\n",
      "---\n",
      "üìú Doc 3:\n",
      "\n",
      "---\n",
      "{\n",
      "  \"answer\": \"I don't know.\",\n",
      "  \"explanation\": \"The model did not return a structured JSON response.\",\n",
      "  \"source_documents\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def load_pdf_and_create_vector_store(pdf_path):\n",
    "    \"\"\"Loads a PDF file, creates embeddings, and stores them in FAISS.\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # ‚úÖ Debugging: Check if PDF loaded correctly\n",
    "    print(f\"üìÑ Loaded {len(docs)} documents from PDF.\")\n",
    "    for i, doc in enumerate(docs[:3]):  # Print first 3 documents for verification\n",
    "        print(f\"üîπ Document {i+1}:\\n{doc.page_content[:500]}\\n---\")\n",
    "\n",
    "    if not docs:\n",
    "        raise ValueError(\"üö® No documents were loaded from the PDF! Check if the file is valid.\")\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"deepseek-llm:7b-chat\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return docs, vectorstore  # Return both docs and vectorstore\n",
    "\n",
    "def query_agent(question: str, docs, vectorstore, llm):\n",
    "    \"\"\"Queries the vectorstore using the LLM and retrieves the answer.\"\"\"\n",
    "    try:\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # ‚úÖ Fetch more documents\n",
    "        retrieved_docs = retriever.invoke(question)  # ‚úÖ Use `invoke()` instead of deprecated method\n",
    "\n",
    "        # ‚úÖ Debugging: Print retrieved documents\n",
    "        print(f\"üîé Retrieved {len(retrieved_docs)} documents for query: {question}\")\n",
    "        for i, doc in enumerate(retrieved_docs[:3]):  # Print first 3 for verification\n",
    "            print(f\"üìú Doc {i+1}:\\n{doc.page_content[:500]}\\n---\")\n",
    "\n",
    "        # If no relevant documents retrieved, fallback to full PDF\n",
    "        if not retrieved_docs:\n",
    "            print(\"‚ö†Ô∏è No relevant documents found in vector search. Using entire PDF content as fallback.\")\n",
    "\n",
    "            # Use entire PDF as context\n",
    "            context = \"\\n\".join([doc.page_content for doc in docs if doc.page_content.strip()])\n",
    "        else:\n",
    "            context = \"\\n\".join([doc.page_content for doc in retrieved_docs if doc.page_content.strip()])\n",
    "\n",
    "        QA_PROMPT = PromptTemplate(\n",
    "            template=\"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say 'I don't know'.\n",
    "\n",
    "            Context: {context}\n",
    "            Question: {query}\n",
    "\n",
    "            Provide the answer in JSON format:\n",
    "            {{\n",
    "                \"answer\": \"your concise answer here\",\n",
    "                \"explanation\": \"your detailed explanation here\"\n",
    "            }}\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "        )\n",
    "    \n",
    "\n",
    "        formatted_prompt = QA_PROMPT.format(context=context, query=question)\n",
    "\n",
    "        # ‚úÖ Send to LLM and ensure JSON parsing\n",
    "        response = llm.invoke(formatted_prompt).strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_response = {\n",
    "                \"answer\": response,\n",
    "                \"explanation\": \"The model did not return a structured JSON response.\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"answer\": parsed_response.get(\"answer\", \"No answer found\"),\n",
    "            \"explanation\": parsed_response.get(\"explanation\", \"No explanation provided\"),\n",
    "            \"source_documents\": [doc.page_content for doc in retrieved_docs if doc.page_content.strip()]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"Error occurred while processing the query\",\n",
    "            \"explanation\": str(e),\n",
    "            \"source_documents\": []\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"English.pdf\"  # Make sure this path is correct\n",
    "    docs, vectorstore = load_pdf_and_create_vector_store(pdf_path)\n",
    "\n",
    "    llm = OllamaLLM(model=\"deepseek-llm:7b-chat\")\n",
    "\n",
    "    query = \"Write down the summary of the poem *Stopping by the Woods on a Snowy Evening* by Robert Frost\"\n",
    "    result = query_agent(query, docs, vectorstore, llm)\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
