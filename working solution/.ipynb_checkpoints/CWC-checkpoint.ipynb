{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-llm:7b-chat' base_url=None client_kwargs={} mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Store the conversation history globally or in a variable\n",
    "conversation_history = []\n",
    "\n",
    "def load_pdf_and_create_vector_store(pdf_path):\n",
    "    \"\"\"Loads a PDF file, creates embeddings, and stores them in FAISS.\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    if not docs:\n",
    "        raise ValueError(\"üö® No documents were loaded from the PDF! Check if the file is valid.\")\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"deepseek-llm:7b-chat\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    # print(embeddings)\n",
    "    print(vectorstore)\n",
    "    return vectorstore\n",
    "\n",
    "pdf_path = \"English.pdf\"  # Make sure this path is correct\n",
    "vectorstore = load_pdf_and_create_vector_store(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"Hi! I am an AI assistant that can help you with various questions and queries. If there's anything specific you would like to know, please feel free to ask and I will do my best to provide a helpful answer based on the available context.\",\n",
      "  \"explanation\": \"The model did not return a structured JSON response.\",\n",
      "  \"source_documents\": [],\n",
      "  \"context\": [\n",
      "    {\n",
      "      \"question\": \"please convert the answer in to markdown with headings and also a little lenghy\",\n",
      "      \"answer\": \"Hi! I am an AI assistant that can help you with various questions and queries. If there's anything specific you would like to know, please feel free to ask and I will do my best to provide a helpful answer based on the available context.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def query_agent(question: str, vectorstore, llm):\n",
    "    \"\"\"Queries the vectorstore using the LLM and retrieves the answer.\"\"\"\n",
    "    try:\n",
    "        # Retrieve documents relevant to the current query\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # Fetch more documents\n",
    "        retrieved_docs = retriever.invoke(question)  # Use `invoke()` instead of deprecated method\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"I don't know.\",\n",
    "                \"explanation\": \"No relevant information found in the documents.\",\n",
    "                \"source_documents\": []\n",
    "            }\n",
    "\n",
    "        context = \"\\n\".join([doc.page_content for doc in retrieved_docs if doc.page_content.strip()])\n",
    "\n",
    "        # Append previous questions and answers to the context\n",
    "        global conversation_history\n",
    "        if conversation_history:\n",
    "            # Join previous conversation history into the context\n",
    "            conversation_context = \"\\n\".join([f\"Q: {entry['question']}\\nA: {entry['answer']}\" for entry in conversation_history])\n",
    "            context = conversation_context + \"\\n\" + context\n",
    "\n",
    "        # QA_PROMPT = PromptTemplate(\n",
    "        #     template=\"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say 'I don't know'.\n",
    "\n",
    "        #     Context: {context}\n",
    "        #     Question: {query}\n",
    "\n",
    "        #     Provide the answer in JSON format:\n",
    "        #     {{\n",
    "        #         \"answer\": \"your concise answer here\",\n",
    "        #         \"explanation\": \"your detailed explanation here\"\n",
    "        #     }}\n",
    "        #     \"\"\",\n",
    "        #     input_variables=[\"context\", \"query\"],\n",
    "        # )\n",
    "\n",
    "        QA_PROMPT = PromptTemplate(\n",
    "            template=\"\"\"You are a helpful assistant that answers questions based on the provided context. If the user asks to modify your last answer (for example, convert it to markdown), you must follow those instructions and apply them to the most recent response.\n",
    "\n",
    "            Context: {context}\n",
    "            Question: {query}\n",
    "\n",
    "            If the question asks for formatting changes, such as markdown, use headings, bullet points, and proper markdown syntax. If you're unsure, ask the user for more clarity.\n",
    "\n",
    "            Provide the answer in JSON format:\n",
    "            {{\n",
    "                \"answer\": \"your concise answer here\",\n",
    "                \"explanation\": \"your detailed explanation here\"\n",
    "            }}\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "        )\n",
    "\n",
    "\n",
    "        formatted_prompt = QA_PROMPT.format(context=context, query=question)\n",
    "\n",
    "        # Send to LLM and ensure JSON parsing\n",
    "        response = llm.invoke(formatted_prompt).strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_response = {\n",
    "                \"answer\": response,\n",
    "                \"explanation\": \"The model did not return a structured JSON response.\"\n",
    "            }\n",
    "\n",
    "        # Save the current question and answer to conversation history\n",
    "        conversation_history.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": parsed_response.get(\"answer\", \"No answer found\"),\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            \"answer\": parsed_response.get(\"answer\", \"No answer found\"),\n",
    "            \"explanation\": parsed_response.get(\"explanation\", \"No explanation provided\"),\n",
    "            \"source_documents\": [doc.page_content for doc in retrieved_docs if doc.page_content.strip()],\n",
    "            \"context\": conversation_history\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"Error occurred while processing the query\",\n",
    "            \"explanation\": str(e),\n",
    "            \"source_documents\": []\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm = OllamaLLM(model=\"deepseek-llm:7b-chat\")\n",
    "\n",
    "    query = \"please convert the answer in to markdown with headings and also a little lenghy\"\n",
    "    result = query_agent(query, vectorstore, llm)\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
