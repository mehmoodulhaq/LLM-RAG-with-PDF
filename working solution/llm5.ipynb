{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Missing some input keys: {'query', 'source_documents'}\n",
      "{\n",
      "  \"answer\": \"Error occurred while processing the query\",\n",
      "  \"explanation\": \"Missing some input keys: {'query', 'source_documents'}\",\n",
      "  \"source_documents\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def load_pdf_and_create_vector_store(pdf_path):\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    embeddings = OllamaEmbeddings(model=\"deepseek-llm:7b-chat\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def query_agent(question: str, vectorstore, llm):\n",
    "    try:\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        QA_PROMPT = PromptTemplate(\n",
    "            template=\"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say 'I don't know'.\n",
    "\n",
    "            Context: {context}\n",
    "            Question: {query}\n",
    "\n",
    "            Answer in JSON format:\n",
    "            {{\n",
    "                \"answer\": \"your concise answer here\",\n",
    "                \"explanation\": \"your detailed explanation here\",\n",
    "                \"source_documents\": \"{source_documents}\"\n",
    "            }}\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\", \"query\", \"source_documents\"],\n",
    "        )\n",
    "\n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,  # Keep this True\n",
    "            chain_type_kwargs={\"prompt\": QA_PROMPT},  # Set the prompt here\n",
    "        )\n",
    "        \n",
    "        response = qa({\"query\": question})\n",
    "\n",
    "        try:\n",
    "            # Access and parse the result correctly\n",
    "            result = json.loads(response['result'])\n",
    "            return result\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            print(f\"Raw response: {response['result']}\") # Print raw response for debugging\n",
    "            return {\n",
    "                \"answer\": \"Could not parse JSON response\",\n",
    "                \"explanation\": response['result'],\n",
    "                \"source_documents\": response.get('source_documents', [])\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"Error occurred while processing the query\",\n",
    "            \"explanation\": str(e),\n",
    "            \"source_documents\": []\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"English.pdf\"\n",
    "    vectorstore = load_pdf_and_create_vector_store(pdf_path)\n",
    "\n",
    "    llm = OllamaLLM(model=\"deepseek-llm:7b-chat\")\n",
    "\n",
    "    query = \"What is the role of counselling in preventing drug addiction?\"\n",
    "    result = query_agent(query, vectorstore, llm)\n",
    "    print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 148 documents from PDF.\n",
      "üîπ Document 1:\n",
      "\n",
      "---\n",
      "üîπ Document 2:\n",
      "\n",
      "---\n",
      "üîπ Document 3:\n",
      "\n",
      "---\n",
      "üîé Retrieved 5 documents for query: What is the role of counselling in preventing drug addiction?\n",
      "üìú Doc 1:\n",
      "\n",
      "---\n",
      "üìú Doc 2:\n",
      "\n",
      "---\n",
      "üìú Doc 3:\n",
      "\n",
      "---\n",
      "{\n",
      "  \"answer\": \"Counselling plays a role in preventing drug addiction by helping individuals identify and change negative thought patterns, cope with stressors, develop problem-solving skills, and improve relationships.\",\n",
      "  \"explanation\": \"Counseling provides support, guidance, and education to help people overcome substance abuse disorders. It helps them understand the causes of their addiction and equips them with coping mechanisms to prevent relapse. Additionally, it facilitates personal growth by improving communication skills, building resilience, and promoting healthy behaviors.\",\n",
      "  \"source_documents\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def load_pdf_and_create_vector_store(pdf_path):\n",
    "    \"\"\"Loads a PDF file, creates embeddings, and stores them in FAISS.\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # ‚úÖ Debugging: Check if PDF loaded correctly\n",
    "    print(f\"üìÑ Loaded {len(docs)} documents from PDF.\")\n",
    "    for i, doc in enumerate(docs[:3]):  # Print first 3 documents for verification\n",
    "        print(f\"üîπ Document {i+1}:\\n{doc.page_content[:500]}\\n---\")\n",
    "\n",
    "    if not docs:\n",
    "        raise ValueError(\"üö® No documents were loaded from the PDF! Check if the file is valid.\")\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"deepseek-llm:7b-chat\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def query_agent(question: str, vectorstore, llm):\n",
    "    \"\"\"Queries the vectorstore using the LLM and retrieves the answer.\"\"\"\n",
    "    try:\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # ‚úÖ Fetch more documents\n",
    "        retrieved_docs = retriever.invoke(question)  # ‚úÖ Use `invoke()` instead of deprecated method\n",
    "\n",
    "        # ‚úÖ Debugging: Print retrieved documents\n",
    "        print(f\"üîé Retrieved {len(retrieved_docs)} documents for query: {question}\")\n",
    "        for i, doc in enumerate(retrieved_docs[:3]):  # Print first 3 for verification\n",
    "            print(f\"üìú Doc {i+1}:\\n{doc.page_content[:500]}\\n---\")\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"I don't know.\",\n",
    "                \"explanation\": \"No relevant information found in the documents.\",\n",
    "                \"source_documents\": []\n",
    "            }\n",
    "\n",
    "        context = \"\\n\".join([doc.page_content for doc in retrieved_docs if doc.page_content.strip()])\n",
    "\n",
    "        QA_PROMPT = PromptTemplate(\n",
    "            template=\"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say 'I don't know'.\n",
    "\n",
    "            Context: {context}\n",
    "            Question: {query}\n",
    "\n",
    "            Provide the answer in JSON format:\n",
    "            {{\n",
    "                \"answer\": \"your concise answer here\",\n",
    "                \"explanation\": \"your detailed explanation here\"\n",
    "            }}\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "        )\n",
    "\n",
    "        formatted_prompt = QA_PROMPT.format(context=context, query=question)\n",
    "\n",
    "        # ‚úÖ Send to LLM and ensure JSON parsing\n",
    "        response = llm.invoke(formatted_prompt).strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_response = {\n",
    "                \"answer\": response,\n",
    "                \"explanation\": \"The model did not return a structured JSON response.\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"answer\": parsed_response.get(\"answer\", \"No answer found\"),\n",
    "            \"explanation\": parsed_response.get(\"explanation\", \"No explanation provided\"),\n",
    "            \"source_documents\": [doc.page_content for doc in retrieved_docs if doc.page_content.strip()]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"Error occurred while processing the query\",\n",
    "            \"explanation\": str(e),\n",
    "            \"source_documents\": []\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"English.pdf\"  # Make sure this path is correct\n",
    "    vectorstore = load_pdf_and_create_vector_store(pdf_path)\n",
    "\n",
    "    llm = OllamaLLM(model=\"deepseek-llm:7b-chat\")\n",
    "\n",
    "    query = \"What is the role of counselling in preventing drug addiction?\"\n",
    "    result = query_agent(query, vectorstore, llm)\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No readable text found. Attempting OCR on the PDF...\n"
     ]
    },
    {
     "ename": "TesseractNotFoundError",
     "evalue": "tesseract is not installed or it's not in your PATH. See README file for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytesseract/pytesseract.py:275\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msubprocess_args())\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[1;32m   1032\u001b[0m                         restore_signals,\n\u001b[1;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/subprocess.py:1955\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tesseract'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTesseractNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    112\u001b[0m     pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Ensure correct path\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     vectorstore \u001b[38;5;241m=\u001b[39m load_pdf_and_create_vector_store(pdf_path)\n\u001b[1;32m    115\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OllamaLLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-llm:7b-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the role of counselling in preventing drug addiction?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[17], line 38\u001b[0m, in \u001b[0;36mload_pdf_and_create_vector_store\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(doc\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ö†Ô∏è No readable text found. Attempting OCR on the PDF...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     text \u001b[38;5;241m=\u001b[39m extract_text_with_ocr(pdf_path)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Create a single document with OCR text\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     docs \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m: text}]\n",
      "Cell \u001b[0;32mIn[17], line 24\u001b[0m, in \u001b[0;36mextract_text_with_ocr\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m     pix \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mget_pixmap()\n\u001b[1;32m     23\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfrombytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m, [pix\u001b[38;5;241m.\u001b[39mwidth, pix\u001b[38;5;241m.\u001b[39mheight], pix\u001b[38;5;241m.\u001b[39msamples)\n\u001b[0;32m---> 24\u001b[0m     text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pytesseract\u001b[38;5;241m.\u001b[39mimage_to_string(img)  \u001b[38;5;66;03m# OCR text extraction\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytesseract/pytesseract.py:486\u001b[0m, in \u001b[0;36mimage_to_string\u001b[0;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    487\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[1;32m    488\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs)},\n\u001b[1;32m    489\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs),\n\u001b[1;32m    490\u001b[0m }[output_type]()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytesseract/pytesseract.py:489\u001b[0m, in \u001b[0;36mimage_to_string.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03mReturns the result of a Tesseract OCR run on the provided image to string\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    484\u001b[0m args \u001b[38;5;241m=\u001b[39m [image, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m, lang, config, nice, timeout]\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    487\u001b[0m     Output\u001b[38;5;241m.\u001b[39mBYTES: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39m(args \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m])),\n\u001b[1;32m    488\u001b[0m     Output\u001b[38;5;241m.\u001b[39mDICT: \u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs)},\n\u001b[0;32m--> 489\u001b[0m     Output\u001b[38;5;241m.\u001b[39mSTRING: \u001b[38;5;28;01mlambda\u001b[39;00m: run_and_get_output(\u001b[38;5;241m*\u001b[39margs),\n\u001b[1;32m    490\u001b[0m }[output_type]()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytesseract/pytesseract.py:352\u001b[0m, in \u001b[0;36mrun_and_get_output\u001b[0;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m save(image) \u001b[38;5;28;01mas\u001b[39;00m (temp_name, input_filename):\n\u001b[1;32m    342\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_filename\u001b[39m\u001b[38;5;124m'\u001b[39m: input_filename,\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m: temp_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[1;32m    350\u001b[0m     }\n\u001b[0;32m--> 352\u001b[0m     run_tesseract(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_output(\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_filename_base\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextsep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mextension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    355\u001b[0m         return_bytes,\n\u001b[1;32m    356\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pytesseract/pytesseract.py:280\u001b[0m, in \u001b[0;36mrun_tesseract\u001b[0;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TesseractNotFoundError()\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timeout_manager(proc, timeout) \u001b[38;5;28;01mas\u001b[39;00m error_string:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode:\n",
      "\u001b[0;31mTesseractNotFoundError\u001b[0m: tesseract is not installed or it's not in your PATH. See README file for more information."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def extract_text_with_ocr(pdf_path):\n",
    "    \"\"\"Uses OCR to extract text from a scanned PDF.\"\"\"\n",
    "    import fitz  # PyMuPDF\n",
    "    \n",
    "    # Open the PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Extract text from each page\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        pix = page.get_pixmap()\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        text += pytesseract.image_to_string(img)  # OCR text extraction\n",
    "        \n",
    "    return text.strip()\n",
    "\n",
    "def load_pdf_and_create_vector_store(pdf_path):\n",
    "    \"\"\"Load the PDF file, apply OCR if needed, and create a vector store.\"\"\"\n",
    "    \n",
    "    # First, try normal PDF extraction with PyMuPDFLoader\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # If no valid documents were extracted, try OCR on the PDF\n",
    "    if not any(doc.page_content.strip() for doc in docs):\n",
    "        print(\"‚ö†Ô∏è No readable text found. Attempting OCR on the PDF...\")\n",
    "        text = extract_text_with_ocr(pdf_path)\n",
    "        \n",
    "        # Create a single document with OCR text\n",
    "        docs = [{\"page_content\": text}]\n",
    "        \n",
    "        if not text.strip():\n",
    "            raise ValueError(\"üö® No valid text extracted even with OCR!\")\n",
    "    \n",
    "    print(f\"üìÑ Loaded {len(docs)} valid documents.\")\n",
    "    embeddings = OllamaEmbeddings(model=\"deepseek-llm:7b-chat\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def query_agent(question: str, vectorstore, llm):\n",
    "    \"\"\"Queries the vectorstore using the LLM and retrieves the answer.\"\"\"\n",
    "    try:\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "        retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "        # Filter out empty documents again\n",
    "        retrieved_docs = [doc for doc in retrieved_docs if doc.page_content.strip()]\n",
    "\n",
    "        print(f\"üîé Retrieved {len(retrieved_docs)} valid documents for query: {question}\")\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"I don't know.\",\n",
    "                \"explanation\": \"No relevant information found in the documents.\",\n",
    "                \"source_documents\": []\n",
    "            }\n",
    "\n",
    "        context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        QA_PROMPT = PromptTemplate(\n",
    "            template=\"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say 'I don't know'.\n",
    "\n",
    "            Context: {context}\n",
    "            Question: {query}\n",
    "\n",
    "            Provide the answer in JSON format:\n",
    "            {{\n",
    "                \"answer\": \"your concise answer here\",\n",
    "                \"explanation\": \"your detailed explanation here\"\n",
    "            }}\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "        )\n",
    "\n",
    "        formatted_prompt = QA_PROMPT.format(context=context, query=question)\n",
    "        response = llm.invoke(formatted_prompt).strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_response = {\n",
    "                \"answer\": response,\n",
    "                \"explanation\": \"The model did not return a structured JSON response.\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"answer\": parsed_response.get(\"answer\", \"No answer found\"),\n",
    "            \"explanation\": parsed_response.get(\"explanation\", \"No explanation provided\"),\n",
    "            \"source_documents\": [doc.page_content for doc in retrieved_docs]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"Error occurred while processing the query\",\n",
    "            \"explanation\": str(e),\n",
    "            \"source_documents\": []\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"English.pdf\"  # Ensure correct path\n",
    "    vectorstore = load_pdf_and_create_vector_store(pdf_path)\n",
    "\n",
    "    llm = OllamaLLM(model=\"deepseek-llm:7b-chat\")\n",
    "\n",
    "    query = \"What is the role of counselling in preventing drug addiction?\"\n",
    "    result = query_agent(query, vectorstore, llm)\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
