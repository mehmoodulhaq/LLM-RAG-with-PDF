{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loaded 148 documents from PDF.\n",
      "🔹 Document 1:\n",
      "\n",
      "---\n",
      "🔹 Document 2:\n",
      "\n",
      "---\n",
      "🔹 Document 3:\n",
      "\n",
      "---\n",
      "🔎 Retrieved 5 documents for query: How can we become a strong nation? Please do not add any new line character symbols\n",
      "📜 Doc 1:\n",
      "\n",
      "---\n",
      "📜 Doc 2:\n",
      "\n",
      "---\n",
      "📜 Doc 3:\n",
      "\n",
      "---\n",
      "{\n",
      "  \"answer\": \"Focus on building a strong economy, investing in education and infrastructure, promoting unity among citizens, and engaging with international partnerships.\",\n",
      "  \"explanation\": \"A strong nation is built upon several key pillars. Firstly, economic growth is crucial to ensure the stability and well-being of its citizens. This can be achieved by fostering innovation, attracting investment, and creating job opportunities. Secondly, investing in education allows a nation to develop its human capital, promoting critical thinking and problem-solving skills.\",\n",
      "  \"source_documents\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def load_pdf_and_create_vector_store(pdf_path):\n",
    "    \"\"\"Loads a PDF file, creates embeddings, and stores them in FAISS.\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # ✅ Debugging: Check if PDF loaded correctly\n",
    "    print(f\"📄 Loaded {len(docs)} documents from PDF.\")\n",
    "    for i, doc in enumerate(docs[:3]):  # Print first 3 documents for verification\n",
    "        print(f\"🔹 Document {i+1}:\\n{doc.page_content[:500]}\\n---\")\n",
    "\n",
    "    if not docs:\n",
    "        raise ValueError(\"🚨 No documents were loaded from the PDF! Check if the file is valid.\")\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"deepseek-llm:7b-chat\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def query_agent(question: str, vectorstore, llm):\n",
    "    \"\"\"Queries the vectorstore using the LLM and retrieves the answer.\"\"\"\n",
    "    try:\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # ✅ Fetch more documents\n",
    "        retrieved_docs = retriever.invoke(question)  # ✅ Use `invoke()` instead of deprecated method\n",
    "\n",
    "        # ✅ Debugging: Print retrieved documents\n",
    "        print(f\"🔎 Retrieved {len(retrieved_docs)} documents for query: {question}\")\n",
    "        for i, doc in enumerate(retrieved_docs[:3]):  # Print first 3 for verification\n",
    "            print(f\"📜 Doc {i+1}:\\n{doc.page_content[:500]}\\n---\")\n",
    "\n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"I don't know.\",\n",
    "                \"explanation\": \"No relevant information found in the documents.\",\n",
    "                \"source_documents\": []\n",
    "            }\n",
    "\n",
    "        context = \"\\n\".join([doc.page_content for doc in retrieved_docs if doc.page_content.strip()])\n",
    "\n",
    "        QA_PROMPT = PromptTemplate(\n",
    "            template=\"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say 'I don't know'.\n",
    "\n",
    "            Context: {context}\n",
    "            Question: {query}\n",
    "\n",
    "            Provide the answer in JSON format:\n",
    "            {{\n",
    "                \"answer\": \"your concise answer here\",\n",
    "                \"explanation\": \"your detailed explanation here\"\n",
    "            }}\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "        )\n",
    "\n",
    "        formatted_prompt = QA_PROMPT.format(context=context, query=question)\n",
    "\n",
    "        # ✅ Send to LLM and ensure JSON parsing\n",
    "        response = llm.invoke(formatted_prompt).strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_response = {\n",
    "                \"answer\": response,\n",
    "                \"explanation\": \"The model did not return a structured JSON response.\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"answer\": parsed_response.get(\"answer\", \"No answer found\"),\n",
    "            \"explanation\": parsed_response.get(\"explanation\", \"No explanation provided\"),\n",
    "            \"source_documents\": [doc.page_content for doc in retrieved_docs if doc.page_content.strip()]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"Error occurred while processing the query\",\n",
    "            \"explanation\": str(e),\n",
    "            \"source_documents\": []\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"English.pdf\"  # Make sure this path is correct\n",
    "    vectorstore = load_pdf_and_create_vector_store(pdf_path)\n",
    "\n",
    "    llm = OllamaLLM(model=\"deepseek-llm:7b-chat\")\n",
    "\n",
    "    query = \"How can we become a strong nation? Please do not add any new line character symbols\"\n",
    "    result = query_agent(query, vectorstore, llm)\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File path English.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m     pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Make sure this path is correct\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     docs, vectorstore \u001b[38;5;241m=\u001b[39m load_pdf_and_create_vector_store(pdf_path)\n\u001b[1;32m     93\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OllamaLLM(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-llm:7b-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite down the summary of the poem *Stopping by the Woods on a Snowy Evening* by Robert Frost\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mload_pdf_and_create_vector_store\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_pdf_and_create_vector_store\u001b[39m(pdf_path):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a PDF file, creates embeddings, and stores them in FAISS.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     loader \u001b[38;5;241m=\u001b[39m PyPDFLoader(pdf_path)\n\u001b[1;32m     12\u001b[0m     docs \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# ✅ Debugging: Check if PDF loaded correctly\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_community/document_loaders/pdf.py:281\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[0;34m(self, file_path, password, headers, extract_images, mode, images_parser, images_inner_format, pages_delimiter, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     file_path: Union[\u001b[38;5;28mstr\u001b[39m, PurePath],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m     extraction_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    251\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize with a file path.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m        `aload` methods to retrieve parsed documents with content and metadata.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(file_path, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(\n\u001b[1;32m    283\u001b[0m         password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[1;32m    284\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m         extraction_kwargs\u001b[38;5;241m=\u001b[39mextraction_kwargs,\n\u001b[1;32m    291\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_community/document_loaders/pdf.py:140\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[0;31mValueError\u001b[0m: File path English.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def load_pdf_and_create_vector_store(pdf_path):\n",
    "    \"\"\"Loads a PDF file, creates embeddings, and stores them in FAISS.\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # ✅ Debugging: Check if PDF loaded correctly\n",
    "    print(f\"📄 Loaded {len(docs)} documents from PDF.\")\n",
    "    for i, doc in enumerate(docs[:3]):  # Print first 3 documents for verification\n",
    "        print(f\"🔹 Document {i+1}:\\n{doc.page_content[:500]}\\n---\")\n",
    "\n",
    "    if not docs:\n",
    "        raise ValueError(\"🚨 No documents were loaded from the PDF! Check if the file is valid.\")\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"deepseek-llm:7b-chat\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return docs, vectorstore  # Return both docs and vectorstore\n",
    "\n",
    "def query_agent(question: str, docs, vectorstore, llm):\n",
    "    \"\"\"Queries the vectorstore using the LLM and retrieves the answer.\"\"\"\n",
    "    try:\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # ✅ Fetch more documents\n",
    "        retrieved_docs = retriever.invoke(question)  # ✅ Use `invoke()` instead of deprecated method\n",
    "\n",
    "        # ✅ Debugging: Print retrieved documents\n",
    "        print(f\"🔎 Retrieved {len(retrieved_docs)} documents for query: {question}\")\n",
    "        for i, doc in enumerate(retrieved_docs[:3]):  # Print first 3 for verification\n",
    "            print(f\"📜 Doc {i+1}:\\n{doc.page_content[:500]}\\n---\")\n",
    "\n",
    "        # If no relevant documents retrieved, fallback to full PDF\n",
    "        if not retrieved_docs:\n",
    "            print(\"⚠️ No relevant documents found in vector search. Using entire PDF content as fallback.\")\n",
    "\n",
    "            # Use entire PDF as context\n",
    "            context = \"\\n\".join([doc.page_content for doc in docs if doc.page_content.strip()])\n",
    "        else:\n",
    "            context = \"\\n\".join([doc.page_content for doc in retrieved_docs if doc.page_content.strip()])\n",
    "\n",
    "        QA_PROMPT = PromptTemplate(\n",
    "            template=\"\"\"Use the following context to answer the question. If the context doesn't contain the answer, say 'I don't know'.\n",
    "\n",
    "            Context: {context}\n",
    "            Question: {query}\n",
    "\n",
    "            Provide the answer in JSON format:\n",
    "            {{\n",
    "                \"answer\": \"your concise answer here\",\n",
    "                \"explanation\": \"your detailed explanation here\"\n",
    "            }}\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\", \"query\"],\n",
    "        )\n",
    "    \n",
    "\n",
    "        formatted_prompt = QA_PROMPT.format(context=context, query=question)\n",
    "\n",
    "        # ✅ Send to LLM and ensure JSON parsing\n",
    "        response = llm.invoke(formatted_prompt).strip()\n",
    "\n",
    "        try:\n",
    "            parsed_response = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed_response = {\n",
    "                \"answer\": response,\n",
    "                \"explanation\": \"The model did not return a structured JSON response.\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"answer\": parsed_response.get(\"answer\", \"No answer found\"),\n",
    "            \"explanation\": parsed_response.get(\"explanation\", \"No explanation provided\"),\n",
    "            \"source_documents\": [doc.page_content for doc in retrieved_docs if doc.page_content.strip()]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        return {\n",
    "            \"answer\": \"Error occurred while processing the query\",\n",
    "            \"explanation\": str(e),\n",
    "            \"source_documents\": []\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"./English.pdf\"  # Make sure this path is correct\n",
    "    docs, vectorstore = load_pdf_and_create_vector_store(pdf_path)\n",
    "\n",
    "    llm = OllamaLLM(model=\"deepseek-llm:7b-chat\")\n",
    "\n",
    "    query = \"Write down the summary of the poem *Stopping by the Woods on a Snowy Evening* by Robert Frost\"\n",
    "    result = query_agent(query, docs, vectorstore, llm)\n",
    "    print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
